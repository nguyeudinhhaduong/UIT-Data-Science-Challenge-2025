{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12956570,"sourceType":"datasetVersion","datasetId":8199948},{"sourceId":12966982,"sourceType":"datasetVersion","datasetId":8206756},{"sourceId":12967200,"sourceType":"datasetVersion","datasetId":8206895},{"sourceId":12985037,"sourceType":"datasetVersion","datasetId":8218820},{"sourceId":13086481,"sourceType":"datasetVersion","datasetId":8288749}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wandb\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwnb_token  = user_secrets.get_secret(\"wandb\")\nwnb_name = 'hallu'\nwandb.login(key=wnb_token)\nwandb.init(name=wnb_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T04:11:27.103228Z","iopub.execute_input":"2025-09-18T04:11:27.103481Z","iopub.status.idle":"2025-09-18T04:11:50.265267Z","shell.execute_reply.started":"2025-09-18T04:11:27.103439Z","shell.execute_reply":"2025-09-18T04:11:50.264463Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.4)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\nRequirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhaduong058a\u001b[0m (\u001b[33mhaduong058a-hcmussh-edu-vn\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250918_041142-vhpxrp6c</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/haduong058a-hcmussh-edu-vn/uncategorized/runs/vhpxrp6c' target=\"_blank\">hallu</a></strong> to <a href='https://wandb.ai/haduong058a-hcmussh-edu-vn/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/haduong058a-hcmussh-edu-vn/uncategorized' target=\"_blank\">https://wandb.ai/haduong058a-hcmussh-edu-vn/uncategorized</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/haduong058a-hcmussh-edu-vn/uncategorized/runs/vhpxrp6c' target=\"_blank\">https://wandb.ai/haduong058a-hcmussh-edu-vn/uncategorized/runs/vhpxrp6c</a>"},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/haduong058a-hcmussh-edu-vn/uncategorized/runs/vhpxrp6c?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x786a84ebac50>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification,\n    TrainingArguments, \n    Trainer,\n    DataCollatorWithPadding\n)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport os, json\n\n# -----------------------------\n# Dataset class\n# -----------------------------\nclass VIHalluDataset(Dataset):\n    def __init__(self, contexts, questions, responses, labels, tokenizer, max_length=512):\n        self.contexts = contexts\n        self.questions = questions\n        self.responses = responses\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        context = str(self.contexts[idx])\n        question = str(self.questions[idx])\n        response = str(self.responses[idx])\n        \n        # Ghép input\n        input_text = f\"{question} [SEP] {context} [SEP] {response}\"\n        \n        encoding = self.tokenizer(\n            input_text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        \n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\n# -----------------------------\n# Metrics\n# -----------------------------\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = np.argmax(predictions, axis=1)\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc}\n\n# -----------------------------\n# Main pipeline\n# -----------------------------\ndef main():\n    # Dùng model mới\n    model_name = \"timpal0l/mdeberta-v3-base-squad2\"\n    train_path = \"/kaggle/input/vihallu-train/vihallu-train.csv\"\n    val_path = \"//kaggle/input/d/nguyendinhhaduong/vihallu/vihallu-warmup.csv\"\n    output_dir = \"/kaggle/working/vihallu_model\"\n\n    # Load data\n    print(\"Loading data...\")\n    df_train = pd.read_csv(train_path).dropna()\n    df_val = pd.read_csv(val_path).dropna()\n\n    # Column mapping\n    contexts_train = df_train[\"context\"].tolist() if \"context\" in df_train.columns else df_train[\"document\"].tolist()\n    questions_train = df_train[\"question\"].tolist() if \"question\" in df_train.columns else df_train[\"prompt\"].tolist()\n    responses_train = df_train[\"response\"].tolist() if \"response\" in df_train.columns else df_train[\"answer\"].tolist()\n    labels_train = [str(l).lower() for l in df_train[\"label\"].tolist()]\n\n    contexts_val = df_val[\"context\"].tolist() if \"context\" in df_val.columns else df_val[\"document\"].tolist()\n    questions_val = df_val[\"question\"].tolist() if \"question\" in df_val.columns else df_val[\"prompt\"].tolist()\n    responses_val = df_val[\"response\"].tolist() if \"response\" in df_val.columns else df_val[\"answer\"].tolist()\n    labels_val = [str(l).lower() for l in df_val[\"label\"].tolist()]\n\n    # Encode labels\n    label_encoder = LabelEncoder()\n    all_labels = labels_train + labels_val\n    label_encoder.fit(all_labels)\n    y_train = label_encoder.transform(labels_train)\n    y_val = label_encoder.transform(labels_val)\n\n    # Tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.add_special_tokens({'sep_token': '[SEP]'})\n\n    # Dataset\n    print(\"Tokenizing training data...\")\n    train_dataset = VIHalluDataset(contexts_train, questions_train, responses_train, y_train, tokenizer)\n    for _ in tqdm(train_dataset, desc=\"Train samples\", total=len(train_dataset)):\n        pass\n\n    print(\"Tokenizing validation data...\")\n    val_dataset = VIHalluDataset(contexts_val, questions_val, responses_val, y_val, tokenizer)\n    for _ in tqdm(val_dataset, desc=\"Val samples\", total=len(val_dataset)):\n        pass\n\n    # Model\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=len(label_encoder.classes_),\n        ignore_mismatched_sizes=True\n    )\n    model.resize_token_embeddings(len(tokenizer))\n\n    # Training args\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=5,\n        per_device_train_batch_size=1,   # batch size\n        per_device_eval_batch_size=8,\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_accuracy\",\n        logging_dir=f\"{output_dir}/logs\",\n        logging_steps=100,\n        save_total_limit=2,\n        learning_rate=2e-5,\n        report_to='wandb',\n        logging_strategy=\"steps\"\n    )\n\n    data_collator = DataCollatorWithPadding(tokenizer)\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics\n    )\n\n    # Train\n    print(\"Starting training...\")\n    trainer.train()\n    trainer.save_model(output_dir)\n\n    # Evaluate\n    predictions = trainer.predict(val_dataset)\n    preds = np.argmax(predictions.predictions, axis=1)\n\n    true_labels = predictions.label_ids\n    acc = accuracy_score(true_labels, preds)\n    report = classification_report(\n        label_encoder.inverse_transform(true_labels),\n        label_encoder.inverse_transform(preds),\n        target_names=label_encoder.classes_\n    )\n    cm = confusion_matrix(\n        label_encoder.inverse_transform(true_labels),\n        label_encoder.inverse_transform(preds),\n        labels=label_encoder.classes_\n    )\n\n    print(f\"Validation Accuracy: {acc:.4f}\")\n    print(report)\n\n    # Confusion matrix plot\n    os.makedirs(output_dir, exist_ok=True)\n    plt.figure(figsize=(8,6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n                xticklabels=label_encoder.classes_,\n                yticklabels=label_encoder.classes_)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"Confusion Matrix\")\n    plt.savefig(f\"{output_dir}/confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n\n    # Save results\n    results = {\n        \"accuracy\": acc,\n        \"classification_report\": report,\n        \"preds\": label_encoder.inverse_transform(preds).tolist(),\n        \"true_labels\": label_encoder.inverse_transform(true_labels).tolist()\n    }\n    with open(f\"{output_dir}/eval_results.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(results, f, indent=2, ensure_ascii=False)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2025-09-18T04:15:08.39781Z","iopub.execute_input":"2025-09-18T04:15:08.398631Z","execution_failed":"2025-09-18T04:21:19.828Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2025-09-18 04:15:29.764284: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758168930.160242      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758168930.268665      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/455 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d80f8be8f86742628aa748bb6709fd07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5992aeeb72b4448480dee972eb4b2742"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd193bab1e2c4d649569631526bba025"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6c291658b024d5d993621d79b4339e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5134fd18bf294e8e9c0d0484f4ec83cb"}},"metadata":{}},{"name":"stdout","text":"Tokenizing training data...\n","output_type":"stream"},{"name":"stderr","text":"Train samples: 100%|██████████| 7000/7000 [00:09<00:00, 704.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Tokenizing validation data...\n","output_type":"stream"},{"name":"stderr","text":"Val samples: 100%|██████████| 198/198 [00:00<00:00, 757.33it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/961 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"737549c0ae3045028e72f5591792867b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.74G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee89b790d7246678a6a39d0e7e11535"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64ef174a82dc4ee98a85c4cb2ea6dc68"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at mathislucka/deberta-large-hallucination-eval-v3 and are newly initialized because the shapes did not match:\n- classifier.weight: found shape torch.Size([1, 1024]) in the checkpoint and torch.Size([3, 1024]) in the model instantiated\n- classifier.bias: found shape torch.Size([1]) in the checkpoint and torch.Size([3]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/3112997833.py:141: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='287' max='17500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  287/17500 04:54 < 4:56:15, 0.97 it/s, Epoch 0.08/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"main()","metadata":{}}]}